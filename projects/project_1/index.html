<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding Handwritten Exam Answers from Scanned Documents | Rahul Krishna Kolayikkath </title> <meta name="author" content="Rahul Krishna Kolayikkath"> <meta name="description" content="Segmenting and associating handwritten student answers with questions in scanned exam booklets using vision-language models and spatial reasoning."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rahulkolayikkath.github.io/projects/project_1/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Rahul Krishna Kolayikkath </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="https://rahulkolayikkath.github.io/personal-website/resume.pdf">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding Handwritten Exam Answers from Scanned Documents</h1> <p class="post-description">Segmenting and associating handwritten student answers with questions in scanned exam booklets using vision-language models and spatial reasoning.</p> </header> <article> <h2 id="overview">Overview</h2> <p>Handwritten document understanding remains a challenging problem, especially in educational settings where structure is weak and variability is high. In this project, I explored how to reliably <strong>segment and label student answers</strong> from scanned, multi-page handwritten exam booklets.</p> <p>The core challenge was: Given a PDF containing multiple pages of handwritten answersâ€”where students may answer questions in arbitrary order, continue answers across pages, and write without strict spatial structureâ€”how can we accurately isolate each answer and associate it with the correct question?</p> <h2 id="problem-setting">Problem Setting</h2> <p>I worked under a few realistic assumptions commonly seen in exam settings:</p> <ul> <li>Students write <strong>question numbers</strong> along the left margin</li> <li>Minimal spacing is left between consecutive answers</li> <li> <strong>Student metadata</strong> (name, roll number) appears on the first page</li> <li>Sub-questions (e.g., <em>1.a</em>, <em>1.b</em>) are common</li> </ul> <p>In practice, however, handwriting quality varies significantly, answers may spill across pages, and marginal or off-boundary writing is frequentâ€”making segmentation difficult.</p> <p>Traditional OCR and layout-based approaches struggle due to:</p> <ul> <li>Noisy handwriting</li> <li>Inconsistent spacing</li> <li>Weak or missing visual boundaries</li> </ul> <hr> <h2 id="initial-attempts-and-limitations">Initial Attempts and Limitations</h2> <p>The first approach relied on a <strong>layout detection model combined with OCR</strong>. The pipeline attempted to:</p> <ul> <li>Detect question numbers, answer blocks, and student metadata as regions</li> <li>Use heuristics to map answers to questions This approach failed in multiple ways:</li> <li>OCR struggled to reliably recognize handwritten question numbers</li> <li>Answers spanning multiple pages broke one-to-one mappings</li> <li>Irregular spacing caused layout models to fragment answers</li> <li>Sub-questions confused both detection and mapping logic</li> </ul> <p>These failures highlighted a key limitation: <strong>purely spatial reasoning is brittle for real handwritten data</strong>.</p> <hr> <h2 id="incorporating-vision-language-models">Incorporating Vision-Language Models</h2> <p>In the second iteration, I introduced a <strong>vision-language model</strong> to extract higher-level semantic information directly from page images.</p> <p>This significantly improved:</p> <ul> <li>Student metadata extraction</li> <li>Question number ordering</li> <li>Detection of answer continuations across pages</li> </ul> <p>However, segmentation still depended on layout detection, which remained unreliableâ€”especially for sub-questions and loosely structured answers.</p> <hr> <h2 id="final-approach-vision--spatial-reasoning">Final Approach: Vision + Spatial Reasoning</h2> <p>The final system replaced traditional layout detection with an <strong>image-understanding model capable of visual grounding</strong>.</p> <p>Key idea:</p> <ol> <li>Use a vision-language model to infer the <strong>list and order of question numbers</strong> </li> <li>Prompt the image model to <strong>localize those specific question numbers</strong> on the page</li> <li>Use the coordinates of successive question numbers to dynamically construct bounding boxes enclosing each answer</li> </ol> <p>This semanticâ€“spatial collaboration proved far more robust:</p> <ul> <li>Answers spanning multiple pages were handled naturally</li> <li>Sub-questions were grouped correctly</li> <li>Marginal and off-boundary writing was captured</li> <li>No rigid layout assumptions were required</li> </ul> <p>The only persistent failure cases involved <strong>extremely ambiguous handwriting</strong> (e.g., â€˜3â€™ written like â€˜8â€™), which are documented as known limitations.</p> <hr> <h2 id="outcome-and-takeaways">Outcome and Takeaways</h2> <p>Improved accuracy on this segmentation task. This exploration demonstrated we can make use of collaboration between between molmo and gemini to accurately draw bounding boxes around answers and map it to correct question numbers Rather than forcing handwritten data into predefined layouts, the system adapts to the structure inferred directly from contentâ€”leading to significantly improved robustness in real-world exam data.</p> <hr> <h2 id="code">Code</h2> <p>ðŸ‘‰ <strong><a href="https://github.com/rahulkolayikkath/handwritten-exam-parser" rel="external nofollow noopener" target="_blank">GitHub Repository Link</a></strong></p> <h2 id="references">References</h2> <p>[1] Molmo, A family of open state-of-the-art multimodal AI models, <a href="https://allenai.org/blog/molmo" rel="external nofollow noopener" target="_blank">Blog</a></p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2026 Rahul Krishna Kolayikkath. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>